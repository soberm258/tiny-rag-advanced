本目录用于做检索侧评测，目标是让检索链路的调参有可复现的客观指标，而不是只靠人工看输出。评测强调的是可自动判定的标准答案，因此样本的 gold 会直接来源于建库产物 `split_sentence.jsonl` 中的结构化元数据字段，而不是让大模型对答案质量打分。这样做的好处是当你更换融合策略、调整权重、修改召回数时，指标变化可以直接归因到检索环节本身。

当前只先覆盖 law 库。评测集生成脚本 `eval/build_law_eval_set.py` 会从 `data/db/law/split_sentence.jsonl` 抽样法条，并为每条法条生成若干条语义型问题，这类问题尽量避免出现条号并减少原文复制，从而更接近“向量检索是否理解语义”。如果你需要衡量 BM25 的上限或系统健康度，可以在生成时开启定位型问题，这类问题允许包含条号，用于验证索引字段与字面检索能力是否正常。

检索评测脚本 `eval/run_law_retrieval_eval.py` 支持用多组实验参数做 A/B 对比，实验参数与 `script/rag_cli.py` 的 search 参数保持同一风格，例如融合方法、BM25 与向量权重、召回倍数等。评测输出只包含检索指标与耗时分解，不会自动运行最终回答生成，你可以在确定检索侧最优参数后再接入后续生成链路。

